---
title: Resources for ML interviews 
date: "2025-01-01"
tags: [interview-prep]
---

During my interview preparation days, I revised and re-revised on standard ML interview topics several times, and found the following resources to be very helpful.

## Overview of different Optimizers

[Detailed and up-to-date post by Sebastian Ruder on optimizers](https://www.ruder.io/optimizing-gradient-descent/)

[Playlist of 5 min videos on Optimizers](https://www.youtube.com/playlist?list=PLreVlKwe2Z0TIZL8Vyfcdw3gKRlB3evGX)

## Similarity metrics

[A blog on cosine similarity](https://brenocon.com/blog/2012/03/cosine-similarity-pearson-correlation-and-ols-coefficients/)

## Evaluation metrics

- NDCG, Recall, Precision, MRR
- BLEU, METEOR, ROUGE, Perplexity

[Recommendation/IR metrics](https://www.evidentlyai.com/ranking-metrics/evaluating-recommender-systems)
[NLG metrics](https://saschametzger.com/blog/exploring-different-natural-language-generation-metrics)

## Transformer nitty-gritties

[Harvard SEAS blog on building transformer from scratch](https://nlp.seas.harvard.edu/annotated-transformer/)

[Wonderful set of notebooks + explanations](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.html)

[Insightful blog on positional embeddings](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/)

## Classical ML

### KNN:

https://kevinzakka.github.io/2016/07/13/k-nearest-neighbor/

### Maximum Likelihood Estimation

https://leimao.github.io/blog/Maximum-Likelihood-Estimation-VS-Maximum-A-Posteriori-Estimation/

https://leimao.github.io/blog/Cross-Entropy-KL-Divergence-MLE/

https://bjlkeng.io/posts/probabilistic-interpretation-of-regularization/

### KMeans

https://antoinebrl.github.io/blog/kmeans/

### SVM

[Detailed explanation on SVM from Prof. Kilian Weinberger](https://www.cs.cornell.edu/courses/cs4780/2017sp/lectures/lecturenote09.html)

### Decision Tree

[Detailed explanation on Decision trees from Prof. Kilian Weinberger](https://www.cs.cornell.edu/courses/cs4780/2017sp/lectures/lecturenote17.html)

### Random Forest

[Detailed explanation on Random Forests from Prof. Kilian Weinberger](https://www.cs.cornell.edu/courses/cs4780/2017sp/lectures/lecturenote18.html)

### Boosting

[An article on gradient boosting works](https://nicolas-hug.com/blog/gradient_boosting_descent)

### SVD

https://www.youtube.com/watch?v=Ed6CSJbyVak

https://www.youtube.com/watch?v=HAJey9-Q8js

https://www.youtube.com/watch?v=KTKAp9Q3yWg

### PCA

https://www.youtube.com/watch?v=pmG4K79DUoI

https://www.youtube.com/watch?v=dhK8nbtii6I

### Collaborative Filtering

A series of articles on Collaborative filtering by [Nicholas Hug](https://nicolas-hug.com/)

[Article 1](https://nicolas-hug.com/blog/matrix_facto_1)

[Article 2](https://nicolas-hug.com/blog/matrix_facto_2)

[Article 3](https://nicolas-hug.com/blog/matrix_facto_3)

[Article 4](https://nicolas-hug.com/blog/matrix_facto_4)


### Approximate Nearest Neighbors 

[Medium article on ANN](https://towardsdatascience.com/comprehensive-guide-to-approximate-nearest-neighbors-algorithms-8b94f057d6b6)

## ML Coding notebooks

[KNN](https://colab.research.google.com/drive/1DupuQAlENUHEQ4z9PiiBWJGDoDNnepRQ?usp=sharing)


[KMeans](https://colab.research.google.com/drive/1bu8sxMEnedJbJMBcZdxzS7OJ5w-ZpuZm?usp=sharing)


[Logistic Regression](https://colab.research.google.com/drive/16KZMcp4T98JshlqzY5v885JoxR_DHlZr?usp=sharing)


[Linear Regression](https://colab.research.google.com/drive/1wk15SiOVx8w64iTtfB1w9tZxW7eJD2Vl?usp=sharing)


[Multi Head Attention](https://colab.research.google.com/drive/1g3acncQOC8_bsMXuHn-x3UJKlEEd5FQe?usp=sharing)
